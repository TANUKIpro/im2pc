# Pi3（Pi3X）× SAM 2で「何を作るのか」コンセプト文書

## 1. このプロジェクトで実現したいこと

私たちは、**手持ちの単眼RGB動画（スマホで撮った動画など）から、特定の“物体だけ”を切り出して、多視点情報として統合し、3D形状（点群／必要ならメッシュ）とカメラ軌跡を復元する**ベースライン・パイプラインを構築します。

* 入力：物体をぐるっと撮影した **RGB動画**
* 出力：

  * **物体の3D再構成**（点群 `.ply` を基本、必要に応じてメッシュ化）
  * **撮影時のカメラ姿勢列**（カメラ軌跡）

この段階では「リアルタイム化」や「KVキャッシュ高速化」は目的に含めず、まずは **確実に動く再構成パイプライン**を作ることがゴールです。

---

## 2. なぜ Pi3（Pi3X）と SAM 2 を組み合わせるのか

### SAM 2：動画内の“同じ物体”を安定してマスク追跡する

SAM 2 は **画像・動画のプロンプト可能なセグメンテーション**を提供し、動画では**状態（メモリ）を保持してマスクを伝播**できます。つまり「最初に対象物を指示すれば、その後のフレームでも同じ物体を追い続ける」ための土台になります。 ([GitHub][1])

### Pi3X：マスク済み多視点画像から、姿勢と点群（点マップ）を一括推定する

Pi3X は π³ を拡張したバージョンで、**再構成品質（点群の滑らかさ）、信頼度推定の改善、条件注入（内参・姿勢・深度）、近似的なメートルスケール**などを強化しています。 ([GitHub][2])
このプロジェクトでは、SAM 2 で「背景を落とした物体中心の画像」を作り、Pi3X に渡して **カメラ姿勢＋点群**を得る、という役割分担が明確になります。

---

## 3. システムの全体像（1枚で分かるデータフロー）

1. **動画入力**（RGB）
2. **フレーム抽出**（間引きサンプリング）
3. **SAM 2で対象物マスク生成・伝播**（初回プロンプト→以降追跡） ([GitHub][1])
4. **マスク適用・クロップ**（背景を抑え、物体が画面を占めるよう整形）
5. **Pi3X推論**（多視点入力 → カメラ姿勢列＋点群/点マップ＋信頼度） ([GitHub][2])
6. **点群融合・ノイズ除去**（信頼度でフィルタ、統合、整形）
7. **成果物出力**（点群 `.ply`、カメラ軌跡、ログ）

---

## 4. 想定ユースケース（新規参入者がイメージしやすい例）

* **小物スキャン**：机の上のフィギュアやガジェットをスマホで一周撮影 → 3D点群化
* **検品・記録**：製品個体の形状を簡易的に記録し、比較や可視化に回す
* **データ作り**：物体3D再構成データを量産し、後段の認識・生成・シミュ用途へ

---

## 5. この段階のスコープ（やること／やらないこと）

### やること（MVP）

* SAM 2 により **対象物マスクを動画全体に伝播**させる（半自動でも可） ([GitHub][1])
* マスク済みフレーム群を Pi3X に入力し、**姿勢列＋点群（点マップ）**を得る ([GitHub][2])
* 信頼度を使って点群を整形し、**再構成結果として保存**する ([GitHub][2])

### やらないこと（今は扱わない）

* リアルタイム処理（高速化・KVキャッシュなど）
* 完全自動の物体選択（“どの物体を追うか”はプロンプト前提）
* 高精度なメートル計測保証（Pi3Xは近似メートルスケール対応だが、MVPでは必須要件にしない） ([GitHub][2])

---

## 6. 成果物（チーム内で共有する“できた”の定義）

最低限、以下が揃っていれば「パイプラインとして成立」とみなします。

* **3D点群ファイル**（例：`.ply`）
* **カメラ姿勢列**（フレーム→4×4行列など）
* **マスク付き中間生成物**（追跡が正しいか検証可能な形で保存）
* **再現可能な実行ログ**（入力動画、サンプリング間隔、使用チェックポイント、主要パラメータ）

---

## 7. 公式リソース（参入者が最初に見るべきもの）

* **Pi3 / Pi3X（GitHub）**：Pi3Xのアップデート内容、推論・入出力の考え方 ([GitHub][2])
* **Pi3（プロジェクトページ）**：全体コンセプトとデモ ([Wang Yifan][3])
* **SAM 2（GitHub）**：動画セグメンテーションの使い方、チェックポイント、VideoPredictorの概念 ([GitHub][1])
* **SAM 2 論文（arXiv）**：モデル背景・想定タスク ([arXiv][4])

---

## 8. この先の拡張（将来の見取り図）

MVPが動いたら、次の方向に拡張できます。

* **完全自動化**：対象物の自動選択、追跡失敗時のリカバリ
* **品質向上**：TSDF融合、メッシュ化、テクスチャ付与
* **オンライン化／高速化**：将来的に KV-Tracker 的な考え方（キャッシュ等）でストリーミング処理へ

[1]: https://github.com/facebookresearch/sam2 "GitHub - facebookresearch/sam2: The repository provides code for running inference with the Meta Segment Anything Model 2 (SAM 2), links for downloading the trained model checkpoints, and example notebooks that show how to use the model."
[2]: https://github.com/yyfz/Pi3 "GitHub - yyfz/Pi3: [ICLR 2026] π^3: Permutation-Equivariant Visual Geometry Learning"
[3]: https://yyfz.github.io/pi3/ "Pi3"
[4]: https://arxiv.org/abs/2408.00714?utm_source=chatgpt.com "SAM 2: Segment Anything in Images and Videos"
